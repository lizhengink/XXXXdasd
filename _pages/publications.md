---
title: ""
permalink: /publications/
author_profile: true
---

<style type="text/css" rel="stylesheet">
.btn--paper {
color: white;
background-color: lightseagreen;
padding: 1px 3px;
text-align: center;
border-radius: 4px;
a { TEXT-DECORATION:none }
}
.btn--arxiv {
color: white;
background-color: tan;
padding: 1px 3px;
text-align: center;
border-radius: 4px;
a { TEXT-DECORATION:none }
}
.btn--code {
color: white;
background-color: DARKORANGE;
padding: 1px 3px;
text-align: center;
border-radius: 4px;
a { TEXT-DECORATION:none }
}
</style>

- Top-tier security venues (9 papers): USENIX-Security ('24,  '23), ACM CCS ('24x3, '23, '22, '21),  NDSS ('23)
- Top-tier machine learning venues (2 papers): ICML ('23), ACL ('23)
- $^*$: Equal contribution; $^\dagger$: Corresponding author

<h2 id='2025'>2025</h2>

### <span style="color:rgb(39, 117, 182)">A Comprehensive Study of Privacy Risks in Curriculum Learning</span>
<font size="3"> Joann Qiongna Chen, Xinlei He, <b>Zheng Li</b>, Yang Zhang, Zhou Li;
<i>PETS 2025</i></font>
<a href="https://zhenglisec.github.io/" class="btn--paper" target="_blank">pdf</a>
<a href="https://arxiv.org/abs/2310.10124" class="btn--arxiv" target="_blank">arxiv</a>
<a href="https://zhenglisec.github.io/" class="btn--code" target="_blank">code</a>

<h2 id='2024'>2024</h2>

### <span style="color:rgb(39, 117, 182)">Membership Inference Attacks Against In-Context Learning</span>
<font size="3"> Rui Wen, <b>Zheng Li$^\dagger$</b>, Michael Backes, Yang Zhang;
<i>CCS 2024</i></font>
<a href="https://zhenglisec.github.io/files/CCS24_ICLMIA.pdf" class="btn--paper" target="_blank">pdf</a>
<a href="https://arxiv.org/abs/2409.01380v1" class="btn--arxiv" target="_blank">arxiv</a>
<a href="https://zhenglisec.github.io/" class="btn--code" target="_blank">code</a>

### <span style="color:rgb(39, 117, 182)">SeqMIA: Sequential-Metric Based Membership Inference Attack</span>
<font size="3"> Hao Li$^*$, <b>Zheng Li$^*$</b>, Siyuan Wu, Chengrui Hu, Yutong Ye, Min Zhang, Dengguo Feng, Yang Zhang;
<i>CCS 2024</i></font>
<a href="https://zhenglisec.github.io/files/CCS24_SeqMIA.pdf" class="btn--paper" target="_blank">pdf</a>
<a href="https://arxiv.org/abs/2407.15098" class="btn--arxiv" target="_blank">arxiv</a>
<a href="https://github.com/AIPAG/SeqMIA" class="btn--code" target="_blank">code</a>

### <span style="color:rgb(39, 117, 182)">BadMerging: Backdoor Attacks Against Model Merging</span>
<font size="3"> Jinghuai Zhang, Jianfeng Chi, <b>Zheng Li</b>, Kunlin Cai, Yang Zhang, Yuan Tian;
<i>CCS 2024</i></font>
<a href="https://zhenglisec.github.io/files/CCS24_BadMerging.pdf" class="btn--paper" target="_blank">pdf</a>
<a href="https://arxiv.org/abs/2408.07362v1" class="btn--arxiv" target="_blank">arxiv</a>
<a href="https://zhenglisec.github.io/" class="btn--code" target="_blank">code</a>

### <span style="color:rgb(39, 117, 182)">SecurityNet: Assessing Machine Learning Vulnerabilities on Public Models</span>
<font size="3"> Boyang Zhang, <b>Zheng Li</b>, Ziqing Yang, Xinlei He, Michael Backes, Mario Fritz, Yang Zhang;
<i>USENIX Security 2024</i></font>
<a href="https://www.usenix.org/system/files/sec24summer-prepub-617-zhang-boyang.pdf" class="btn--paper" target="_blank">pdf</a>
<a href="https://arxiv.org/abs/2310.12665" class="btn--arxiv" target="_blank">arxiv</a>
<a href="https://github.com/SecurityNet-Research/SecurityNet" class="btn--code" target="_blank">code</a>

### <span style="color:rgb(39, 117, 182)">Inside the Black Box: Detecting Data Leakage in Pre-trained Language Encoders</span>
<font size="3"> Yuan Xin, <b>Zheng Li</b>, Ning Yu, Dingfan Chen, Mario Fritz, Michael Backes, Yang Zhang;
<i>ECAI 2024</i></font>
<a href="https://zhenglisec.github.io/" class="btn--paper" target="_blank">pdf</a>
<a href="https://arxiv.org/abs/2408.11046" class="btn--arxiv" target="_blank">arxiv</a>
<a href="https://zhenglisec.github.io/" class="btn--code" target="_blank">code</a>

### <span style="color:rgb(39, 117, 182)">Detection and Attribution of Models Trained on Generated Data</span>
<font size="3"> Ge Han, Ahmed Salem, <b>Zheng Li$^\dagger$</b>, Shanqing Guo, Michael Backes, Yang Zhang;
<i>ICASSP 2024</i></font>
<a href="https://zhenglisec.github.io/" class="btn--paper" target="_blank">pdf</a>
<a href="https://zhenglisec.github.io/" class="btn--arxiv" target="_blank">arxiv</a>
<a href="https://zhenglisec.github.io/" class="btn--code" target="_blank">code</a>

<!-- ### <span style="color:rgb(39, 117, 182)">PRJack: Pruning-Resistant Model Hijacking Attack Against Deep Learning Models</span>
<font size="3"> Ge Han, <b>Zheng Li</b>, Shanqing Guo;
<i>IJCNN 2024</i></font>
<a href="https://zhenglisec.github.io/" class="btn--paper" target="_blank">pdf</a>
<a href="https://zhenglisec.github.io/" class="btn--arxiv" target="_blank">arxiv</a>
<a href="https://zhenglisec.github.io/" class="btn--code" target="_blank">code</a> -->

### <span style="color:rgb(39, 117, 182)">Model Hijacking Attack in Federated Learning</span>
<font size="3"><b>Zheng Li</b>, Siyuan Wu, Ruichuan Chen, Paarijaat Aditya, Istemi Ekin Akkus, Manohar Vanga, Min Zhang, Hao Li, Yang Zhang;</font>
<a href="https://arxiv.org/abs/2408.02131" class="btn--arxiv" target="_blank">arxiv</a>

### <span style="color:rgb(39, 117, 182)">Membership Inference Attack Against Masked Image Modeling</span>
<font size="3"><b>Zheng Li</b>, Xinlei He, Ning Yu, Yang Zhang;</font>
<a href="https://arxiv.org/abs/2408.06825v1" class="btn--arxiv" target="_blank">arxiv</a>

### <span style="color:rgb(39, 117, 182)">Jailbreaking Text-to-Image Models with LLM-Based Agents</span>
<font size="3">Yingkai Dong, <b>Zheng Li</b>, Xiangtao Meng, Ning Yu, Shanqing Guo;</font>
<a href="https://arxiv.org/abs/2408.00523" class="btn--arxiv" target="_blank">arxiv</a>

<!-- ### <span style="color:rgb(39, 117, 182)">Membership Inference Attack Against Masked Image Modeling</span>
<font size="3"><b>Zheng Li</b>, Xinlei He, Ning Yu, Yang Zhang;</font>
<a href="https://arxiv.org/abs/2408.06825v1" class="btn--arxiv" target="_blank">arxiv</a> -->

<!-- ### <span style="color:rgb(39, 117, 182)">PRJack: Pruning-Resistant Model Hijacking Attack Against Deep Learning Models</span>
<font size="3"> Ge Han, <b>Zheng Li$\dagger$</b>, Shanqing Guo;
<i>IJCNN 2024</i></font>
<a href="https://zhenglisec.github.io/" class="btn--paper" target="_blank">pdf</a>
<a href="https://zhenglisec.github.io/" class="btn--arxiv" target="_blank">arxiv</a>
<a href="https://zhenglisec.github.io/" class="btn--code" target="_blank">code</a> -->

<h2 id='2023'>2023</h2>

### <span style="color:rgb(39, 117, 182)">On the Privacy Risks of Machine Learning Models</span>
<font size="3"> <b>Zheng Li</b>;
<i>Ph.D. Dissertation</i></font>
<a href="https://publikationen.sulb.uni-saarland.de/handle/20.500.11880/36610?locale=en" class="btn--paper" target="_blank">pdf</a>
<a href="https://hosting.services.iit.cnr.it/STM-WG/contentpage06.html" class="btn--success" target="_blank">[ERCIM WG STM Best Ph.D. Thesis Award 2024]</a>

### <span style="color:rgb(39, 117, 182)">UnGANable: Defending Against GAN-based Face Manipulation</span>
<font size="3"> <b>Zheng Li</b>, Ning Yu, Ahmed Salem, Michael Backes, Mario Fritz, Yang Zhang;
<i>USENIX Security 2023</i></font>
<a href="https://arxiv.org/abs/2210.00957" class="btn--paper" target="_blank">pdf</a>
<a href="https://arxiv.org/abs/2210.00957" class="btn--arxiv" target="_blank">arxiv</a>
<a href="https://github.com/zhenglisec/UnGANable" class="btn--code" target="_blank">code</a>

### <span style="color:rgb(39, 117, 182)">DE-FAKE: Detection and Attribution of Fake Images Generated by Text-to-Image Generation Models</span>
<font size="3">Zeyang Sha, <b>Zheng Li</b>, Ning Yu, Yang Zhang;
<i>CCS 2023</i></font>
<a href="https://arxiv.org/abs/2210.06998" class="btn--paper" target="_blank">pdf</a>
<a href="https://arxiv.org/abs/2210.06998" class="btn--arxiv" target="_blank">arxiv</a>
<a href="https://arxiv.org/abs/2210.06998" class="btn--code" target="_blank">code</a>

### <span style="color:rgb(39, 117, 182)">Data Poisoning Attacks Against Multimodal Encoders</span>
<font size="3">Ziqing Yang, Xinlei He, <b>Zheng Li</b>, Michael Backes, Mathias Humbert, Pascal Berrang, Yang Zhang;
<i>ICML 2023</i></font>
<a href="https://arxiv.org/abs/2209.15266" class="btn--paper" target="_blank">pdf</a>
<a href="https://arxiv.org/abs/2209.15266" class="btn--arxiv" target="_blank">arxiv</a>
<a href="https://github.com/zqypku/mm_poison/" class="btn--code" target="_blank">code</a>

### <span style="color:rgb(39, 117, 182)">NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models</span>
<font size="3">Kai Mei, <b>Zheng Li</b>, Zhenting Wang, Yang Zhang, Shiqing Ma;
<i>ACL 2023</i></font>
<a href="https://zhenglisec.github.io/" class="btn--paper" target="_blank">pdf</a>
<a href="https://arxiv.org/abs/2305.17826" class="btn--arxiv" target="_blank">arxiv</a>
<a href="https://github.com/RU-System-Software-and-Security/Notable" class="btn--code" target="_blank">code</a>

### <span style="color:rgb(39, 117, 182)">Backdoor Attacks Against Dataset Distillation</span>
<font size="3">Yugeng Liu, <b>Zheng Li</b>, Michael Backes, Yun Shen, Yang Zhang;
<i>NDSS 2023</i></font>
<a href="https://arxiv.org/abs/2301.01197" class="btn--paper" target="_blank">pdf</a>
<a href="https://arxiv.org/abs/2301.01197" class="btn--arxiv" target="_blank">arxiv</a>
<a href="https://github.com/liuyugeng/baadd" class="btn--code" target="_blank">code</a>

### <span style="color:rgb(39, 117, 182)">Watermarking Diffusion Model</span>
<font size="3">Yugeng Liu, <b>Zheng Li</b>, Michael Backes, Yun Shen, Yang Zhang;</font>
<a href="https://arxiv.org/abs/2305.12502" class="btn--arxiv" target="_blank">arxiv</a>

### <span style="color:rgb(39, 117, 182)">Generative Watermarking Against Unauthorized Subject-Driven Image Synthesis</span>
<font size="3">Yihan Ma, Zhengyu Zhao, Xinlei He, <b>Zheng Li</b>, Michael Backes, Yang Zhang;</font>
<a href="https://arxiv.org/abs/2306.07754" class="btn--arxiv" target="_blank">arxiv</a>

<h2 id='2022'>2022</h2>

### <span style="color:rgb(39, 117, 182)">Auditing Membership Leakages of Multi-Exit Networks</span>
<font size="3"><b>Zheng Li</b>, Yiyong Liu, Xinlei He, Ning Yu, Michael Backes, Yang Zhang;
<i>CCS 2022</i></font>
<a href="https://arxiv.org/abs/2208.11180" class="btn--paper" target="_blank">pdf</a>
<a href="https://arxiv.org/abs/2208.11180" class="btn--arxiv" target="_blank">arxiv</a>
<a href="https://github.com/zhenglisec/Multi-Exit-Privacy" class="btn--code" target="_blank">code</a>

### <span style="color:rgb(39, 117, 182)">FuzzGAN: A Generation-Based Fuzzing Framework For Testing Deep Neural Networks</span>
<font size="3">Ge Han, <b>Zheng Li</b>, Peng Tang, Chengyu Hu, Shanqing Guo;
<i>HPCC 2022</i></font>
<a href="https://ieeexplore.ieee.org/document/10074689?denied=" class="btn--paper" target="_blank">pdf</a>
<a href="https://zhenglisec.github.io/" class="btn--arxiv" target="_blank">arxiv</a>
<a href="https://zhenglisec.github.io/" class="btn--code" target="_blank">code</a>

### <span style="color:rgb(39, 117, 182)">Membership-Doctor: Comprehensive Assessment of Membership Inference Against Machine Learning Models</span>
<font size="3">Xinlei He, <b>Zheng Li</b>, Weilin Xu, Cory Cornelius, Yang Zhang;</font>
<a href="https://arxiv.org/abs/2208.10445" class="btn--arxiv" target="_blank">arxiv</a>

### <span style="color:rgb(39, 117, 182)">Membership Inference Attacks Against Text-to-image Generation Models</span>
<font size="3">Yixin Wu, Ning Yu, <b>Zheng Li</b>, Michael Backes, Yang Zhang;</font>
<a href="https://arxiv.org/abs/2210.00968" class="btn--arxiv" target="_blank">arxiv</a>

### <span style="color:rgb(39, 117, 182)">Backdoor Attacks in the Supply Chain of Masked Image Modeling</span>
<font size="3">Xinyue Shen, Xinlei He, <b>Zheng Li</b>, Yun Shen, Michael Backes, Yang Zhang;</font>
<a href="https://arxiv.org/abs/2210.01632" class="btn--arxiv" target="_blank">arxiv</a>


<h2 id='2021'>2021</h2>

### <span style="color:rgb(39, 117, 182)">Membership Leakage in Label-Only Exposures</span>
<font size="3"><b>Zheng Li</b>, Yang Zhang;
<i>CCS 2021</i></font>
<a href="https://arxiv.org/abs/2007.15528" class="btn--paper" target="_blank">pdf</a>
<a href="https://arxiv.org/abs/2007.15528" class="btn--arxiv" target="_blank">arxiv</a>
<a href="https://github.com/zhenglisec/Decision-based-MIA" class="btn--code" target="_blank">code</a>


<h2 id='2019'>2019</h2>

### <span style="color:rgb(39, 117, 182)">How to Prove Your Model Belongs to You: A Blind-Watermark based Framework to Protect Intellectual Property of DNN</span>
<font size="3"><b>Zheng Li</b>,  Chengyu Hu, Yang Zhang, Shanqing Guo;
<i>ACSAC 2019</i></font>
<a href="https://arxiv.org/abs/1903.01743" class="btn--paper" target="_blank">pdf</a>
<a href="https://arxiv.org/abs/1903.01743" class="btn--arxiv" target="_blank">arxiv</a>
<a href="https://github.com/zhenglisec/Blind-Watermark-for-DNN" class="btn--code" target="_blank">code</a>


### <span style="color:rgb(39, 117, 182)">DeepKeyStego: Protecting Communication by Key-dependent Steganography with Deep Networks</span>
<font size="3"><b>Zheng Li</b>,  Ge Han, Shanqing Guo, Chengyu Hu;
<i>HPCC 2019</i></font>
<a href="https://ieeexplore.ieee.org/document/8855704" class="btn--paper" target="_blank">pdf</a>
<a href="https://ieeexplore.ieee.org/document/8855704" class="btn--arxiv" target="_blank">arxiv</a>
<a href="https://github.com/zhenglisec/DeepKeyStego" class="btn--code" target="_blank">code</a>



